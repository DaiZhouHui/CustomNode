#!/usr/bin/env python3
"""
FOFAçˆ¬è™« - GitHub Actionsä¼˜åŒ–ç‰ˆ
æ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œç¯å¢ƒå˜é‡
"""

import requests
import json
import os
import sys
import csv
import re
import base64
import time
import argparse
from datetime import datetime
from urllib.parse import quote

class FOFACrawler:
    def __init__(self, config_file=None):
        """åˆå§‹åŒ–çˆ¬è™«"""
        # ç¡®å®šé…ç½®æ–‡ä»¶è·¯å¾„
        if config_file:
            self.config_file = config_file
        else:
            # å°è¯•åœ¨å½“å‰ç›®å½•æŸ¥æ‰¾config.json
            self.config_file = "config.json"
            
        self.config = self.load_config()
        self.data_found = False
        self.extracted_data = []
        
        # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–Cookie
        env_cookie = os.getenv('FOFA_COOKIE')
        if env_cookie and 'cookies' in self.config:
            self.config['cookies'] = env_cookie
        elif env_cookie:
            self.config['cookies'] = env_cookie
        
        # è®¾ç½®å®Œæ•´è¯·æ±‚å¤´ï¼ˆç”¨äºå¸¦Cookieçš„è¯·æ±‚ï¼‰
        self.full_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'Cache-Control': 'max-age=0',
            'Connection': 'keep-alive',
            'DNT': '1',
            'Referer': 'https://en.fofa.info/',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36 Edg/144.0.0.0',
            'sec-ch-ua': '"Not(A:Brand";v="8", "Chromium";v="144", "Microsoft Edge";v="144"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'Cookie': self.config.get('cookies', '') if 'cookies' in self.config else ''
        }
        
        # è®¾ç½®ç®€å•è¯·æ±‚å¤´ï¼ˆç”¨äºä¸å¸¦Cookieçš„è¯·æ±‚ï¼‰
        self.simple_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        }
        
        # å¸¸è§çš„æ— æ•ˆIPåˆ—è¡¨
        self.invalid_ips = [
            '1.1.1.1', '8.8.8.8', '8.8.4.4', '127.0.0.1', '0.0.0.0', 
            '255.255.255.255', '192.168.0.1', '192.168.1.1', '10.0.0.1',
            '172.16.0.1', '100.64.0.1', '169.254.0.1'
        ]
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not os.path.exists(self.config_file):
            print(f"âŒ é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨")
            print(f"   ä½¿ç”¨é»˜è®¤é…ç½®æˆ–ç¯å¢ƒå˜é‡")
            
            # å°è¯•ä»ç¯å¢ƒå˜é‡æ„å»ºé…ç½®
            query_string = os.getenv('FOFA_QUERY', '')
            cookies = os.getenv('FOFA_COOKIE', '')
            
            if not query_string:
                print("âŒ æœªæ‰¾åˆ°æŸ¥è¯¢è¯­å¥ï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® FOFA_QUERY")
                return {}
            
            return {
                'query_string': query_string,
                'cookies': cookies,
                'settings': {
                    'timeout': 30,
                    'max_results': 10,  # ä¿®æ”¹ä¸º10
                    'filter_common_ips': True,
                    'debug_mode': os.getenv('FOFA_DEBUG', 'false').lower() == 'true'
                }
            }
        
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
            return config
        except Exception as e:
            print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return {}
    
    def encode_query(self, query_string):
        """å°†æŸ¥è¯¢å­—ç¬¦ä¸²ç¼–ç ä¸ºbase64æ ¼å¼"""
        try:
            query_bytes = query_string.encode('utf-8')
            base64_bytes = base64.b64encode(query_bytes)
            base64_string = base64_bytes.decode('utf-8')
            return base64_string
        except Exception as e:
            print(f"âŒ Base64ç¼–ç å¤±è´¥: {e}")
            return None
    
    def build_urls(self):
        """æ„å»ºURLåˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰"""
        urls = []
        
        # è·å–æŸ¥è¯¢å­—ç¬¦ä¸²å¹¶ç¼–ç 
        query_string = self.config.get('query_string', '')
        if not query_string:
            print("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰æŸ¥è¯¢è¯­å¥")
            return urls
        
        base64_query = self.encode_query(query_string)
        if not base64_query:
            return urls
        
        # URLç¼–ç base64å­—ç¬¦ä¸²
        encoded_base64 = quote(base64_query, safe='')
        
        # æŒ‰ç…§æ–°çš„ä¼˜å…ˆçº§æ„å»ºURLåˆ—è¡¨
        # 1. å¸¦å‚æ•°ã€å¸¦cookieå’Œè¯·æ±‚å¤´çš„è‹±æ–‡ç«™ï¼ˆé¦–é€‰ï¼‰
        urls.append({
            'name': 'å¸¦å‚æ•°å’ŒCookieçš„è‹±æ–‡ç«™',
            'url': f"https://en.fofa.info/result?qbase64={encoded_base64}",
            'headers': self.full_headers,
            'has_cookie': True
        })
        
        # 2. å¸¦å‚æ•°çš„è‹±æ–‡ç«™ï¼Œä¸å¸¦cookieå’Œè¯·æ±‚å¤´
        urls.append({
            'name': 'å¸¦å‚æ•°ä¸å¸¦Cookieçš„è‹±æ–‡ç«™',
            'url': f"https://en.fofa.info/result?qbase64={encoded_base64}",
            'headers': self.simple_headers,
            'has_cookie': False
        })
        
        # 3. å¸¦å‚æ•°çš„ä¸­æ–‡ç«™ï¼Œä¸å¸¦cookieå’Œè¯·æ±‚å¤´
        urls.append({
            'name': 'å¸¦å‚æ•°ä¸å¸¦Cookieçš„ä¸­æ–‡ç«™',
            'url': f"https://fofa.info/result?qbase64={encoded_base64}",
            'headers': self.simple_headers,
            'has_cookie': False
        })
        
        print(f"âœ… æ„å»ºäº† {len(urls)} ä¸ªURLï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰")
        for i, url_info in enumerate(urls):
            cookie_status = "æœ‰Cookie" if url_info['has_cookie'] else "æ— Cookie"
            print(f"  {i+1}. {url_info['name']} ({cookie_status})")
            print(f"     URL: {url_info['url'][:80]}...")
        
        return urls
    
    def make_request(self, url_info):
        """å‘é€HTTPè¯·æ±‚"""
        print(f"\nğŸ“¡ å‘é€è¯·æ±‚åˆ°: {url_info['name']}")
        print(f"  URL: {url_info['url']}")
        print(f"  è¯·æ±‚å¤´: {'å®Œæ•´' if url_info['has_cookie'] else 'ç®€å•'}")
        
        try:
            response = requests.get(
                url_info['url'], 
                headers=url_info['headers'], 
                timeout=self.config.get('settings', {}).get('timeout', 30),
                allow_redirects=True
            )
            
            print(f"  âœ… è¯·æ±‚å®Œæˆ!")
            print(f"    çŠ¶æ€ç : {response.status_code}")
            print(f"    å“åº”å¤§å°: {len(response.content)} å­—èŠ‚")
            print(f"    å†…å®¹ç±»å‹: {response.headers.get('Content-Type', 'æœªçŸ¥')}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜è°ƒè¯•HTML
            if self.config.get('settings', {}).get('debug_mode', False):
                self.save_debug_html(response.text, url_info['name'])
            
            if response.status_code == 200:
                return True, response
            elif response.status_code == 403:
                return False, "è®¿é—®è¢«æ‹’ç» (403)"
            elif response.status_code == 401:
                return False, "éœ€è¦è®¤è¯ (401)"
            else:
                return False, f"HTTPé”™è¯¯: {response.status_code}"
                
        except requests.exceptions.Timeout:
            return False, "è¯·æ±‚è¶…æ—¶"
        except requests.exceptions.ConnectionError:
            return False, "è¿æ¥é”™è¯¯"
        except Exception as e:
            return False, f"è¯·æ±‚å¼‚å¸¸: {type(e).__name__}: {str(e)}"
    
    def save_debug_html(self, content, name):
        """ä¿å­˜è°ƒè¯•HTMLæ–‡ä»¶"""
        try:
            debug_dir = "debug"
            if not os.path.exists(debug_dir):
                os.makedirs(debug_dir)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            safe_name = name.replace(" ", "_").replace("/", "_")[:30]
            filename = f"{debug_dir}/{timestamp}_{safe_name}.html"
            
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content[:10000])  # åªä¿å­˜å‰10000å­—ç¬¦
            
            print(f"    ğŸ“ è°ƒè¯•HTMLå·²ä¿å­˜: {filename}")
        except Exception as e:
            print(f"    âš ï¸  ä¿å­˜è°ƒè¯•HTMLå¤±è´¥: {e}")
    
    def extract_table_data(self, html_content):
        """ä»è¡¨æ ¼ä¸­æå–IPå’Œç«¯å£æ•°æ®ï¼ˆä¸»è¦æ–¹æ³•ï¼‰"""
        print("  æ­£åœ¨è§£æè¡¨æ ¼æ•°æ®...")
        
        ip_port_pairs = []
        
        # æ–¹æ³•1: ç›´æ¥åŒ¹é…æ¯ä¸ªæ•°æ®æ¡ç›®ä¸­çš„ä¸‰ä¸ªå…³é”®å…ƒç´ 
        print("    æ–¹æ³•1: ç›´æ¥åŒ¹é…æ•°æ®æ¡ç›®...")
        
        # é¦–å…ˆæ‰¾åˆ°æ‰€æœ‰çš„æ•°æ®æ¡ç›®å®¹å™¨
        # æ¯ä¸ªæ¡ç›®ç”±<div class="hsxa-meta-data-item">å¼€å§‹
        item_pattern = r'<div class="hsxa-meta-data-item">(.*?)</div>\s*</div>\s*</div>\s*</div>'
        items = re.findall(item_pattern, html_content, re.DOTALL)
        
        print(f"      æ‰¾åˆ° {len(items)} ä¸ªæ•°æ®æ¡ç›®")
        
        for item_index, item_html in enumerate(items):
            # åœ¨æ¯ä¸ªæ¡ç›®ä¸­æå–IPã€ç«¯å£å’ŒHOST
            
            # 1. æå–IPåœ°å€
            ip_pattern = r'<a[^>]*href="[^"]*qbase64=aXA=[^"]*"[^>]*class="hsxa-jump-a"[^>]*>([^<]+)</a>'
            ip_matches = re.findall(ip_pattern, item_html, re.DOTALL)
            
            if ip_matches:
                ip = ip_matches[0].strip()
                # æ£€æŸ¥æ˜¯å¦æœ‰éšè—çš„IPï¼ˆdisplay:noneï¼‰
                if len(ip_matches) > 1:
                    for ip_candidate in ip_matches[1:]:
                        if 'display:none' not in ip_candidate and ip_candidate.strip():
                            ip = ip_candidate.strip()
                            break
            else:
                # å¦‚æœæ‰¾ä¸åˆ°hsxa-jump-aï¼Œå°è¯•å…¶ä»–æ¨¡å¼
                ip_pattern2 = r'>\s*(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})\s*<'
                ip_matches2 = re.findall(ip_pattern2, item_html)
                ip = ip_matches2[0].strip() if ip_matches2 else None
            
            # 2. æå–ç«¯å£
            port_pattern = r'<a[^>]*href="[^"]*qbase64=cG9ydD=[^"]*"[^>]*class="hsxa-port"[^>]*>([^<]+)</a>'
            port_matches = re.findall(port_pattern, item_html, re.DOTALL)
            
            if port_matches:
                port = port_matches[0].strip()
            else:
                # å¦‚æœæ‰¾ä¸åˆ°hsxa-portï¼Œå°è¯•å…¶ä»–æ¨¡å¼
                port_pattern2 = r'port[^0-9]*(\d{1,5})'
                port_matches2 = re.search(port_pattern2, item_html, re.IGNORECASE)
                port = port_matches2.group(1) if port_matches2 else "443"
            
            # 3. éªŒè¯IPå¹¶æ·»åŠ åˆ°åˆ—è¡¨
            if ip and self.is_valid_ip(ip):
                # ç¡®ä¿ç«¯å£æ˜¯æœ‰æ•ˆçš„æ•°å­—
                if not port.isdigit():
                    # å°è¯•ä»ç«¯å£ä¸­æå–æ•°å­—
                    port_match = re.search(r'(\d{1,5})', port)
                    port = port_match.group(1) if port_match else "443"
                
                ip_port_pairs.append([ip, port])
                print(f"      æ¡ç›® {item_index+1}: IP={ip}, ç«¯å£={port}")
        
        # æ–¹æ³•2: å¦‚æœæ–¹æ³•1æ²¡æ‰¾åˆ°æ•°æ®ï¼Œå°è¯•é€šç”¨çš„ç»“æ„åŒ–æå–
        if not ip_port_pairs:
            print("    æ–¹æ³•2: å°è¯•é€šç”¨ç»“æ„åŒ–æå–...")
            
            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«IPçš„é“¾æ¥
            all_ip_links = re.findall(r'<a[^>]*href="[^"]*qbase64=aXA=[^"]*"[^>]*>([^<]+)</a>', html_content)
            all_port_links = re.findall(r'<a[^>]*href="[^"]*qbase64=cG9ydD=[^"]*"[^>]*>([^<]+)</a>', html_content)
            
            print(f"      æ‰¾åˆ° {len(all_ip_links)} ä¸ªIPé“¾æ¥, {len(all_port_links)} ä¸ªç«¯å£é“¾æ¥")
            
            # å‡è®¾IPå’Œç«¯å£æ˜¯æŒ‰é¡ºåºå¯¹åº”çš„
            min_count = min(len(all_ip_links), len(all_port_links))
            for i in range(min_count):
                ip = all_ip_links[i].strip()
                port = all_port_links[i].strip()
                
                if self.is_valid_ip(ip):
                    if not port.isdigit():
                        port_match = re.search(r'(\d{1,5})', port)
                        port = port_match.group(1) if port_match else "443"
                    
                    ip_port_pairs.append([ip, port])
        
        # æ–¹æ³•3: æå–hosté“¾æ¥ä¸­çš„IP
        if not ip_port_pairs:
            print("    æ–¹æ³•3: æå–hosté“¾æ¥...")
            
            # æŸ¥æ‰¾hosté“¾æ¥
            host_pattern = r'<a[^>]*href="(https?://[^"]*)"[^>]*target="_blank"[^>]*>[^<]*<i[^>]*class="[^"]*icon-link[^"]*"[^>]*>'
            host_matches = re.findall(host_pattern, html_content)
            
            for host_url in host_matches:
                # ä»host URLä¸­æå–IP
                ip_match = re.search(r'https?://([^:/]+)', host_url)
                if ip_match:
                    host = ip_match.group(1)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯IPåœ°å€
                    if self.is_valid_ip(host):
                        # ä»URLä¸­æå–ç«¯å£
                        port_match = re.search(r':(\d+)/?', host_url)
                        port = port_match.group(1) if port_match else "443"
                        ip_port_pairs.append([host, port])
        
        return ip_port_pairs
    
    def is_valid_ip(self, ip_str):
        """éªŒè¯IPåœ°å€æ˜¯å¦æœ‰æ•ˆä¸”ä¸æ˜¯å¸¸è§æ— æ•ˆIP"""
        # åŸºæœ¬IPæ ¼å¼éªŒè¯
        ip_pattern = r'^\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b$'
        if not re.match(ip_pattern, ip_str):
            return False
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ— æ•ˆIPåˆ—è¡¨ä¸­
        if self.config.get('settings', {}).get('filter_common_ips', True):
            if ip_str in self.invalid_ips:
                return False
        
        # æ£€æŸ¥æ¯ä¸ªéƒ¨åˆ†æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
        parts = ip_str.split('.')
        if len(parts) != 4:
            return False
        
        for part in parts:
            if not part.isdigit():
                return False
            num = int(part)
            if num < 0 or num > 255:
                return False
        
        # æ’é™¤ä¸€äº›ç‰¹æ®ŠIPæ®µ
        first_octet = int(parts[0])
        if first_octet == 0:  # 0.x.x.x
            return False
        if first_octet == 10:  # 10.x.x.x (å†…ç½‘)
            return False
        if first_octet == 100 and 64 <= int(parts[1]) <= 127:  # 100.64.x.x-100.127.x.x (è¿è¥å•†NAT)
            return False
        if first_octet == 127:  # 127.x.x.x (ç¯å›)
            return False
        if first_octet == 169 and int(parts[1]) == 254:  # 169.254.x.x (é“¾è·¯æœ¬åœ°)
            return False
        if first_octet == 172 and 16 <= int(parts[1]) <= 31:  # 172.16.x.x-172.31.x.x (å†…ç½‘)
            return False
        if first_octet == 192 and int(parts[1]) == 168:  # 192.168.x.x (å†…ç½‘)
            return False
        if first_octet == 198 and 18 <= int(parts[1]) <= 19:  # 198.18.x.x-198.19.x.x (æµ‹è¯•)
            return False
        
        return True
    
    def extract_data_from_response(self, response):
        """ä»å“åº”ä¸­æå–IPå’Œç«¯å£æ•°æ®"""
        print("\nğŸ” æ­£åœ¨æå–æ•°æ®...")
        
        html_content = response.text
        
        # æ–¹æ³•1: å°è¯•è¡¨æ ¼è§£æï¼ˆæ”¹è¿›ç‰ˆï¼‰
        ip_port_pairs = self.extract_table_data(html_content)
        
        # æ–¹æ³•2: å¦‚æœè¡¨æ ¼è§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
        if not ip_port_pairs:
            print("  è¡¨æ ¼è§£ææœªæ‰¾åˆ°æ•°æ®ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•...")
            
            # å¤‡ç”¨æ–¹æ³•1: ç›´æ¥æŸ¥æ‰¾æ‰€æœ‰IPå’Œç«¯å£
            ip_pattern = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b'
            all_ips = re.findall(ip_pattern, html_content)
            
            # è¿‡æ»¤å’ŒéªŒè¯IP
            valid_ips = []
            for ip in all_ips:
                if self.is_valid_ip(ip):
                    if ip not in valid_ips:
                        valid_ips.append(ip)
            
            print(f"    æ‰¾åˆ° {len(all_ips)} ä¸ªIPï¼Œè¿‡æ»¤å {len(valid_ips)} ä¸ªæœ‰æ•ˆIP")
            
            # ä¸ºæ¯ä¸ªIPåˆ†é…ç«¯å£
            for ip in valid_ips[:self.config.get('settings', {}).get('max_results', 10)]:
                # åœ¨IPé™„è¿‘æŸ¥æ‰¾ç«¯å£
                ip_index = html_content.find(ip)
                if ip_index != -1:
                    # æŸ¥çœ‹IPå‰å200å­—ç¬¦
                    start = max(0, ip_index - 200)
                    end = min(len(html_content), ip_index + 200)
                    context = html_content[start:end]
                    
                    # æŸ¥æ‰¾ç«¯å£
                    port = "443"  # é»˜è®¤ç«¯å£
                    
                    # å°è¯•å¤šç§æ–¹å¼æŸ¥æ‰¾ç«¯å£
                    port_patterns = [
                        r'port[^0-9]*(\d{1,5})',
                        r'ç«¯å£[^0-9]*(\d{1,5})',
                        r':(\d{1,5})/',
                        r'>(\d{1,5})<'
                    ]
                    
                    for pattern in port_patterns:
                        port_match = re.search(pattern, context, re.IGNORECASE)
                        if port_match:
                            port_candidate = port_match.group(1)
                            if 1 <= int(port_candidate) <= 65535:
                                port = port_candidate
                                break
                    
                    ip_port_pairs.append([ip, port])
        
        # å»é‡
        unique_pairs = []
        seen = set()
        
        for pair in ip_port_pairs:
            key = tuple(pair)
            if key not in seen:
                seen.add(key)
                unique_pairs.append(pair)
        
        print(f"  æ‰¾åˆ° {len(ip_port_pairs)} ä¸ªIPç«¯å£å¯¹ï¼Œå»é‡å {len(unique_pairs)} ä¸ª")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªç»“æœ
        if unique_pairs:
            print(f"\n  æ•°æ®é¢„è§ˆ (å‰{min(10, len(unique_pairs))}æ¡):")
            for i, pair in enumerate(unique_pairs[:10]):
                print(f"    {i+1:2d}. IP: {pair[0]:15s} ç«¯å£: {pair[1]}")
        
        return unique_pairs
    
    def save_to_csv(self, data, output_file=None):
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        if not data:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return False
        
        if not output_file:
            output_file = "results.csv"
        
        try:
            with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(['IPåœ°å€', 'ç«¯å£'])
                writer.writerows(data)
            
            print(f"\nâœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            print(f"   å…±ä¿å­˜ {len(data)} æ¡è®°å½•")
            
            # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
            print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
            print("-" * 40)
            print(f"æ€»è®°å½•æ•°: {len(data)}")
            if len(data) > 0:
                print(f"ç¬¬ä¸€æ¡: IP: {data[0][0]:15s} ç«¯å£: {data[0][1]}")
                print(f"æœ€åä¸€æ¡: IP: {data[-1][0]:15s} ç«¯å£: {data[-1][1]}")
            print("-" * 40)
            
            return True
        except Exception as e:
            print(f"\nâŒ ä¿å­˜CSVå¤±è´¥: {e}")
            return False
    
    def run(self):
        """è¿è¡Œçˆ¬è™«ä¸»é€»è¾‘"""
        print("=" * 60)
        print("FOFAçˆ¬è™« v3.0 - GitHub Actionsä¼˜åŒ–ç‰ˆ")
        print("=" * 60)
        
        # æ£€æŸ¥é…ç½®
        if not self.config:
            print("âŒ é…ç½®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡")
            return False
        
        if 'cookies' not in self.config or not self.config['cookies']:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°Cookieï¼Œéƒ¨åˆ†URLå¯èƒ½æ— æ³•è®¿é—®")
        
        # æ˜¾ç¤ºæŸ¥è¯¢è¯­å¥
        query_string = self.config.get('query_string', '')
        print(f"æŸ¥è¯¢è¯­å¥: {query_string}")
        
        # æ„å»ºURLåˆ—è¡¨
        urls = self.build_urls()
        if not urls:
            print("âŒ æ— æ³•æ„å»ºURLï¼Œè¯·æ£€æŸ¥æŸ¥è¯¢è¯­å¥")
            return False
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•URL
        for url_info in urls:
            print(f"\n{'='*60}")
            print(f"å°è¯•: {url_info['name']}")
            print(f"{'='*60}")
            
            success, response = self.make_request(url_info)
            
            if success:
                # æå–æ•°æ®
                data = self.extract_data_from_response(response)
                
                if data:
                    print(f"  âœ… ä» {url_info['name']} æˆåŠŸæå–åˆ° {len(data)} æ¡æ•°æ®")
                    self.data_found = True
                    self.extracted_data = data
                    
                    # è·å–åˆ°æ•°æ®åç«‹å³åœæ­¢
                    print(f"\nâœ… å·²æˆåŠŸè·å–æ•°æ®ï¼Œåœæ­¢å°è¯•åç»­URL")
                    break
                else:
                    print(f"  âš ï¸  è¯·æ±‚æˆåŠŸä½†æœªæå–åˆ°æ•°æ®ï¼Œå°è¯•ä¸‹ä¸€ä¸ªURL")
                    # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                    time.sleep(1)
            else:
                print(f"  âŒ è¯·æ±‚å¤±è´¥: {response}")
                # çŸ­æš‚å»¶è¿Ÿ
                time.sleep(1)
        
        # æ€»ç»“
        if self.data_found:
            print("\nğŸ‰ çˆ¬è™«æ‰§è¡ŒæˆåŠŸ!")
            return True
        else:
            print("\nğŸ˜ æ‰€æœ‰URLå°è¯•éƒ½æœªè·å–åˆ°æ•°æ®")
            print("\nå»ºè®®:")
            print("1. æ£€æŸ¥Cookieæ˜¯å¦è¿‡æœŸ")
            print("2. å°è¯•æ›´æ–°Cookie")
            print("3. ç¡®è®¤æŸ¥è¯¢è¯­å¥æ­£ç¡®")
            print("4. æ‰‹åŠ¨è®¿é—®URLç¡®è®¤å¯è®¿é—®æ€§")
            
            return False

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='FOFAçˆ¬è™«å·¥å…·')
    parser.add_argument('--config', default=None, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', default='results.csv', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    crawler = FOFACrawler(args.config)
    
    # å¦‚æœæŒ‡å®šäº†debugæ¨¡å¼ï¼Œè°ƒæ•´é…ç½®
    if args.debug:
        if 'settings' not in crawler.config:
            crawler.config['settings'] = {}
        crawler.config['settings']['debug_mode'] = True
    
    try:
        success = crawler.run()
        
        if success and crawler.extracted_data:
            # ä¿å­˜æ•°æ®åˆ°æŒ‡å®šæ–‡ä»¶
            crawler.save_to_csv(crawler.extracted_data, args.output)
            print("\nğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆ!")
            return 0
        else:
            print("\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥")
            return 1
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        return 1
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¼‚å¸¸: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())