#!/usr/bin/env python3
"""
FOFAçˆ¬è™« - é«˜çº§ååçˆ¬ç‰ˆ
ä¸“é—¨åº”å¯¹FOFAçš„åçˆ¬è™«æœºåˆ¶
"""

import requests
import json
import os
import sys
import csv
import re
import base64
import time
import random
from datetime import datetime
from urllib.parse import quote
import hashlib
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AdvancedFOFACrawler:
    def __init__(self, config_file="config.json"):
        """åˆå§‹åŒ–é«˜çº§çˆ¬è™«"""
        self.config_file = config_file
        self.config = self.load_config()
        self.data_found = False
        self.extracted_data = []
        
        # ä¼šè¯ç®¡ç†
        self.session = requests.Session()
        
        # æµè§ˆå™¨æŒ‡çº¹
        self.browser_fingerprint = self.generate_browser_fingerprint()
        
        # è¯·æ±‚å¤´æ± 
        self.header_pool = self.generate_header_pool()
        
        # ä»£ç†è®¾ç½®
        self.proxy_pool = self.config.get('proxies', [])
        self.current_proxy_index = 0
        
        # è¯·æ±‚ç»Ÿè®¡
        self.request_count = 0
        self.last_request_time = 0
        
        # åˆå§‹åŒ–Cookie
        self.init_cookies()
        
        # åçˆ¬æ£€æµ‹
        self.anti_anti_crawler_settings = {
            'min_delay': self.config.get('settings', {}).get('min_delay', 5),
            'max_delay': self.config.get('settings', {}).get('max_delay', 15),
            'random_mouse_movements': True,
            'random_scrolls': True,
            'human_typing_pattern': True
        }
        
        # ä¿å­˜å“åº”çš„ç›®å½•
        self.debug_dir = "debug_responses"
        if not os.path.exists(self.debug_dir):
            os.makedirs(self.debug_dir)
    
    def generate_browser_fingerprint(self):
        """ç”Ÿæˆæµè§ˆå™¨æŒ‡çº¹"""
        return {
            'screen_resolution': f"{random.randint(1280, 1920)}x{random.randint(720, 1080)}",
            'language': random.choice(['zh-CN', 'en-US', 'en-GB', 'zh-TW']),
            'timezone': random.choice(['Asia/Shanghai', 'America/New_York', 'Europe/London']),
            'platform': random.choice(['Win32', 'Win64', 'MacIntel', 'Linux x86_64']),
            'hardware_concurrency': random.choice([4, 8, 12, 16]),
            'device_memory': random.choice([4, 8, 16]),
            'pixel_ratio': random.choice([1, 1.25, 1.5, 2]),
            'webgl_vendor': random.choice(['NVIDIA Corporation', 'Intel Inc.', 'AMD']),
            'webgl_renderer': random.choice(['NVIDIA GeForce GTX', 'Intel HD Graphics', 'AMD Radeon'])
        }
    
    def generate_header_pool(self):
        """ç”Ÿæˆè¯·æ±‚å¤´æ± """
        return [
            {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
                'Accept-Encoding': 'gzip, deflate, br, zstd',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
                'Sec-Ch-Ua': '"Not/A)Brand";v="8", "Chromium";v="144", "Google Chrome";v="144"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'same-origin',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1',
                'DNT': '1',
                'Connection': 'keep-alive'
            },
            {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:128.0) Gecko/20100101 Firefox/128.0',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
                'Accept-Encoding': 'gzip, deflate, br',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1'
            },
            {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'Cache-Control': 'max-age=0',
                'Connection': 'keep-alive',
                'Sec-Ch-Ua': '"Not/A)Brand";v="8", "Chromium";v="144", "Google Chrome";v="144"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"macOS"',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'same-origin',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1'
            }
        ]
    
    def get_random_headers(self):
        """è·å–éšæœºè¯·æ±‚å¤´"""
        headers = random.choice(self.header_pool)
        
        # æ·»åŠ æµè§ˆå™¨æŒ‡çº¹ä¿¡æ¯
        headers['X-Screen-Resolution'] = self.browser_fingerprint['screen_resolution']
        headers['Accept-Language'] = self.browser_fingerprint['language']
        
        return headers
    
    def get_proxy(self):
        """è·å–ä»£ç†"""
        if not self.proxy_pool:
            return None
        
        proxy = self.proxy_pool[self.current_proxy_index]
        self.current_proxy_index = (self.current_proxy_index + 1) % len(self.proxy_pool)
        
        return {
            'http': proxy,
            'https': proxy
        }
    
    def init_cookies(self):
        """åˆå§‹åŒ–Cookie"""
        cookies_str = self.config.get('cookies', '')
        cookies_dict = self.parse_cookies_string(cookies_str)
        
        # æ›´æ–°ä¼šè¯Cookie
        for key, value in cookies_dict.items():
            self.session.cookies.set(key, value)
    
    def parse_cookies_string(self, cookies_str):
        """è§£æCookieå­—ç¬¦ä¸²ä¸ºå­—å…¸"""
        cookies_dict = {}
        for cookie in cookies_str.split(';'):
            cookie = cookie.strip()
            if '=' in cookie:
                key, value = cookie.split('=', 1)
                cookies_dict[key] = value
        return cookies_dict
    
    def human_like_delay(self):
        """æ¨¡æ‹Ÿäººç±»æ“ä½œçš„å»¶è¿Ÿ"""
        min_delay = self.anti_anti_crawler_settings['min_delay']
        max_delay = self.anti_anti_crawler_settings['max_delay']
        
        # åŸºç¡€å»¶è¿Ÿ
        base_delay = random.uniform(min_delay, max_delay)
        
        # éšæœºæ€è€ƒæ—¶é—´
        thinking_time = random.uniform(0.5, 3.0)
        
        # é¼ æ ‡ç§»åŠ¨æ—¶é—´
        if self.anti_anti_crawler_settings['random_mouse_movements']:
            mouse_movement_time = random.uniform(0.1, 1.5)
        else:
            mouse_movement_time = 0
        
        total_delay = base_delay + thinking_time + mouse_movement_time
        
        current_time = time.time()
        time_since_last_request = current_time - self.last_request_time
        
        if time_since_last_request < total_delay:
            sleep_time = total_delay - time_since_last_request
            logger.info(f"â³ æ¨¡æ‹Ÿäººç±»å»¶è¿Ÿ: ç­‰å¾… {sleep_time:.2f} ç§’")
            time.sleep(sleep_time)
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not os.path.exists(self.config_file):
            logger.error(f"é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨")
            return {}
        
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info("âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
            return config
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return {}
    
    def encode_query(self, query_string):
        """å°†æŸ¥è¯¢å­—ç¬¦ä¸²ç¼–ç ä¸ºbase64æ ¼å¼"""
        try:
            query_bytes = query_string.encode('utf-8')
            base64_bytes = base64.b64encode(query_bytes)
            base64_string = base64_bytes.decode('utf-8')
            return base64_string
        except Exception as e:
            logger.error(f"Base64ç¼–ç å¤±è´¥: {e}")
            return None
    
    def build_urls(self):
        """æ„å»ºURLåˆ—è¡¨"""
        urls = []
        
        query_string = self.config.get('query_string', '')
        if not query_string:
            logger.error("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰æŸ¥è¯¢è¯­å¥")
            return urls
        
        base64_query = self.encode_query(query_string)
        if not base64_query:
            return urls
        
        encoded_base64 = quote(base64_query, safe='')
        
        # ä½¿ç”¨å¤šä¸ªä¸åŒçš„URLæ¨¡å¼
        url_templates = [
            f"https://en.fofa.info/result?qbase64={encoded_base64}",
            f"https://en.fofa.info/result?q={encoded_base64}&qbase64={encoded_base64}",
            f"https://en.fofa.info/result?qbase64={encoded_base64}&page=1&page_size=10"
        ]
        
        for i, template in enumerate(url_templates):
            urls.append({
                'name': f'å°è¯•{i+1}',
                'url': template,
                'priority': i
            })
        
        logger.info(f"âœ… æ„å»ºäº† {len(urls)} ä¸ªURL")
        return urls
    
    def check_anti_crawler(self, response):
        """æ£€æŸ¥åçˆ¬è™«æœºåˆ¶"""
        anti_crawler_indicators = [
            'éªŒè¯ç ', 'captcha', 'è¯·è¾“å…¥éªŒè¯ç ', 'è®¿é—®è¿‡äºé¢‘ç¹',
            'è¯·å…ˆç™»å½•', 'ç™»å½•å¤±æ•ˆ', 'Cookieè¿‡æœŸ', 'äººæœºéªŒè¯',
            'Just a moment...', 'Checking your browser',
            'Security Check', 'Access Denied', 'robot'
        ]
        
        content_lower = response.text.lower()
        
        for indicator in anti_crawler_indicators:
            if indicator.lower() in content_lower:
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ°åçˆ¬è™«æŒ‡ç¤º: {indicator}")
                return True
        
        # æ£€æŸ¥å“åº”é•¿åº¦
        if len(response.text) < 1000:
            logger.warning(f"âš ï¸ å“åº”å†…å®¹è¿‡çŸ­ ({len(response.text)} å­—èŠ‚)ï¼Œå¯èƒ½è¢«æ‹¦æˆª")
            return True
        
        return False
    
    def make_request(self, url_info, attempt=1):
        """å‘é€HTTPè¯·æ±‚"""
        self.request_count += 1
        
        logger.info(f"\nğŸ“¡ å‘é€è¯·æ±‚ #{self.request_count}: {url_info['name']} (å°è¯• {attempt}/3)")
        
        # äººç±»å»¶è¿Ÿ
        self.human_like_delay()
        
        try:
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            headers = self.get_random_headers()
            proxy = self.get_proxy()
            
            # æ·»åŠ Referer
            if random.random() > 0.5:
                headers['Referer'] = 'https://en.fofa.info/'
            
            request_kwargs = {
                'url': url_info['url'],
                'headers': headers,
                'timeout': self.config.get('settings', {}).get('timeout', 30),
                'allow_redirects': True,
                'verify': True  # å¯ç”¨SSLéªŒè¯
            }
            
            if proxy:
                request_kwargs['proxies'] = proxy
                logger.info(f"  ä½¿ç”¨ä»£ç†: {proxy.get('https', proxy.get('http'))}")
            
            # éšæœºé€‰æ‹©GETæˆ–POSTï¼ˆå¤§å¤šæ•°æƒ…å†µæ˜¯GETï¼‰
            if random.random() < 0.1:  # 10%çš„æ¦‚ç‡ä½¿ç”¨POST
                response = self.session.post(**request_kwargs)
                logger.info("  ä½¿ç”¨POSTæ–¹æ³•")
            else:
                response = self.session.get(**request_kwargs)
            
            self.last_request_time = time.time()
            
            logger.info(f"  âœ… è¯·æ±‚å®Œæˆ!")
            logger.info(f"    çŠ¶æ€ç : {response.status_code}")
            logger.info(f"    å“åº”å¤§å°: {len(response.text)} å­—èŠ‚")
            
            # æ£€æŸ¥åçˆ¬è™«
            if self.check_anti_crawler(response):
                if attempt < 3:
                    logger.warning(f"  âš ï¸ æ£€æµ‹åˆ°åçˆ¬è™«ï¼Œç­‰å¾…åé‡è¯•...")
                    time.sleep(random.uniform(10, 30))
                    
                    # åˆ‡æ¢User-Agent
                    self.session.headers.update(self.get_random_headers())
                    
                    return self.make_request(url_info, attempt + 1)
                return False, "åçˆ¬è™«æœºåˆ¶æ£€æµ‹"
            
            if response.status_code == 200:
                return True, response
            elif response.status_code == 429:
                logger.warning(f"  âš ï¸ è¯·æ±‚è¿‡äºé¢‘ç¹ (429)ï¼Œç­‰å¾…é‡è¯•...")
                time.sleep(random.uniform(30, 60))
                if attempt < 3:
                    return self.make_request(url_info, attempt + 1)
                return False, "è¯·æ±‚è¿‡äºé¢‘ç¹"
            elif response.status_code == 403:
                logger.error(f"  âŒ è®¿é—®è¢«æ‹’ç» (403)")
                return False, "è®¿é—®è¢«æ‹’ç»"
            elif response.status_code == 401:
                logger.error(f"  âŒ éœ€è¦è®¤è¯ (401)")
                return False, "éœ€è¦è®¤è¯"
            else:
                logger.error(f"  âŒ HTTPé”™è¯¯: {response.status_code}")
                return False, f"HTTPé”™è¯¯: {response.status_code}"
                
        except requests.exceptions.Timeout:
            logger.error(f"  â° è¯·æ±‚è¶…æ—¶")
            if attempt < 3:
                time.sleep(random.uniform(5, 10))
                return self.make_request(url_info, attempt + 1)
            return False, "è¯·æ±‚è¶…æ—¶"
        except requests.exceptions.ConnectionError:
            logger.error(f"  ğŸ”Œ è¿æ¥é”™è¯¯")
            if attempt < 3:
                time.sleep(random.uniform(10, 20))
                return self.make_request(url_info, attempt + 1)
            return False, "è¿æ¥é”™è¯¯"
        except Exception as e:
            logger.error(f"  âŒ è¯·æ±‚å¼‚å¸¸: {type(e).__name__}: {str(e)}")
            if attempt < 3:
                time.sleep(random.uniform(5, 15))
                return self.make_request(url_info, attempt + 1)
            return False, f"è¯·æ±‚å¼‚å¸¸: {type(e).__name__}"
    
    def extract_data_from_response(self, response):
        """ä»å“åº”ä¸­æå–IPå’Œç«¯å£æ•°æ®"""
        logger.info("\nğŸ” æ­£åœ¨æå–æ•°æ®...")
        
        html_content = response.text
        
        # ä¿å­˜å“åº”ç”¨äºåˆ†æ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        debug_file = f"{self.debug_dir}/response_{timestamp}.html"
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        logger.info(f"  å“åº”å·²ä¿å­˜åˆ°: {debug_file}")
        
        # å¤šç§æå–æ–¹æ³•
        extraction_methods = [
            self.extract_via_host_pattern,
            self.extract_via_clipboard,
            self.extract_via_ip_port_links,
            self.extract_via_regex
        ]
        
        all_pairs = []
        
        for method in extraction_methods:
            pairs = method(html_content)
            if pairs:
                logger.info(f"  æ–¹æ³• {method.__name__} æ‰¾åˆ° {len(pairs)} æ¡æ•°æ®")
                all_pairs.extend(pairs)
                if len(all_pairs) >= self.config.get('settings', {}).get('max_results', 10):
                    break
            else:
                logger.info(f"  æ–¹æ³• {method.__name__} æœªæ‰¾åˆ°æ•°æ®")
        
        # å»é‡
        unique_pairs = []
        seen = set()
        
        for pair in all_pairs:
            key = tuple(pair)
            if key not in seen:
                seen.add(key)
                unique_pairs.append(pair)
        
        logger.info(f"  æ€»å…±æ‰¾åˆ° {len(all_pairs)} ä¸ªIPç«¯å£å¯¹ï¼Œå»é‡å {len(unique_pairs)} ä¸ª")
        
        # é™åˆ¶æœ€å¤§ç»“æœæ•°é‡
        max_results = self.config.get('settings', {}).get('max_results', 10)
        if len(unique_pairs) > max_results:
            unique_pairs = unique_pairs[:max_results]
            logger.info(f"  é™åˆ¶ä¸ºå‰ {max_results} æ¡ç»“æœ")
        
        # æ˜¾ç¤ºç»“æœ
        if unique_pairs:
            logger.info(f"\n  æ•°æ®é¢„è§ˆ:")
            for i, pair in enumerate(unique_pairs):
                logger.info(f"    {i+1:2d}. IP: {pair[0]:15s} ç«¯å£: {pair[1]}")
        else:
            logger.warning("  âš ï¸  æœªæå–åˆ°ä»»ä½•æ•°æ®")
            # å°è¯•ä»ä¿å­˜çš„æ–‡ä»¶ä¸­åˆ†æ
            self.analyze_html_structure(html_content)
        
        return unique_pairs
    
    def extract_via_host_pattern(self, html_content):
        """é€šè¿‡hostæ¨¡å¼æå–"""
        pattern = r'<span class="hsxa-host"[^>]*>\s*<a[^>]*href="[^"]*"[^>]*>([^<]+)</a>'
        matches = re.findall(pattern, html_content)
        
        pairs = []
        for match in matches:
            match = match.strip()
            if ':' in match:
                ip, port = match.split(':', 1)
                if self.is_valid_ip(ip):
                    pairs.append([ip, port])
        
        return pairs
    
    def extract_via_clipboard(self, html_content):
        """é€šè¿‡clipboardæ•°æ®æå–"""
        pattern = r'data-clipboard-text="([^"]+:\d+)"'
        matches = re.findall(pattern, html_content)
        
        pairs = []
        for match in matches:
            if ':' in match:
                ip, port = match.split(':', 1)
                if self.is_valid_ip(ip):
                    pairs.append([ip, port])
        
        return pairs
    
    def extract_via_ip_port_links(self, html_content):
        """é€šè¿‡ç‹¬ç«‹çš„IPå’Œç«¯å£é“¾æ¥æå–"""
        # æå–IP
        ip_pattern = r'<a[^>]*class="hsxa-jump-a"[^>]*href="[^"]*qbase64=aXA=[^"]*"[^>]*>([^<]+)</a>'
        ip_matches = re.findall(ip_pattern, html_content)
        
        # æå–ç«¯å£
        port_pattern = r'<a[^>]*class="hsxa-port"[^>]*href="[^"]*qbase64=cG9ydD=[^"]*"[^>]*>([^<]+)</a>'
        port_matches = re.findall(port_pattern, html_content)
        
        pairs = []
        min_count = min(len(ip_matches), len(port_matches))
        for i in range(min_count):
            ip = ip_matches[i].strip()
            port = port_matches[i].strip()
            
            if self.is_valid_ip(ip):
                if not port.isdigit():
                    port_match = re.search(r'(\d{1,5})', port)
                    port = port_match.group(1) if port_match else "443"
                
                pairs.append([ip, port])
        
        return pairs
    
    def extract_via_regex(self, html_content):
        """é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æå–"""
        # åŒ¹é…IP:ç«¯å£æ ¼å¼
        pattern = r'(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}):(\d{1,5})'
        matches = re.findall(pattern, html_content)
        
        pairs = []
        for ip, port in matches:
            if self.is_valid_ip(ip):
                pairs.append([ip, port])
        
        return pairs
    
    def analyze_html_structure(self, html_content):
        """åˆ†æHTMLç»“æ„"""
        logger.info("  åˆ†æHTMLç»“æ„...")
        
        # æŸ¥æ‰¾å…³é”®å…ƒç´ 
        elements = {
            'hsxa-meta-data-item': html_content.count('hsxa-meta-data-item'),
            'hsxa-host': html_content.count('hsxa-host'),
            'hsxa-jump-a': html_content.count('hsxa-jump-a'),
            'hsxa-port': html_content.count('hsxa-port'),
            'data-clipboard-text': html_content.count('data-clipboard-text'),
            'éªŒè¯ç ': html_content.count('éªŒè¯ç '),
            'captcha': html_content.count('captcha')
        }
        
        for key, value in elements.items():
            if value > 0:
                logger.info(f"    æ‰¾åˆ° {value} ä¸ª '{key}' å…ƒç´ ")
    
    def is_valid_ip(self, ip_str):
        """éªŒè¯IPåœ°å€"""
        # ç®€åŒ–çš„IPéªŒè¯
        pattern = r'^\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b$'
        if not re.match(pattern, ip_str):
            return False
        
        parts = ip_str.split('.')
        if len(parts) != 4:
            return False
        
        for part in parts:
            if not part.isdigit():
                return False
            num = int(part)
            if num < 0 or num > 255:
                return False
        
        return True
    
    def save_to_csv(self, data):
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        if not data:
            logger.error("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return False
        
        output_file = "results.csv"
        
        try:
            with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(['IPåœ°å€', 'ç«¯å£'])
                writer.writerows(data)
            
            logger.info(f"\nâœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            logger.info(f"   å…±ä¿å­˜ {len(data)} æ¡è®°å½•")
            
            return True
        except Exception as e:
            logger.error(f"\nâŒ ä¿å­˜CSVå¤±è´¥: {e}")
            return False
    
    def run(self):
        """è¿è¡Œçˆ¬è™«ä¸»é€»è¾‘"""
        logger.info("=" * 60)
        logger.info(f"FOFAé«˜çº§çˆ¬è™« - ååçˆ¬ç‰ˆ")
        logger.info(f"è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"æµè§ˆå™¨æŒ‡çº¹: {self.browser_fingerprint['platform']}")
        logger.info("=" * 60)
        
        # æ£€æŸ¥é…ç½®
        if not self.config:
            logger.error("âŒ é…ç½®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
            return False
        
        # æ˜¾ç¤ºæŸ¥è¯¢è¯­å¥
        query_string = self.config.get('query_string', '')
        logger.info(f"æŸ¥è¯¢è¯­å¥: {query_string}")
        
        # æ„å»ºURLåˆ—è¡¨
        urls = self.build_urls()
        if not urls:
            logger.error("âŒ æ— æ³•æ„å»ºURLï¼Œè¯·æ£€æŸ¥æŸ¥è¯¢è¯­å¥")
            return False
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•URL
        for url_info in urls:
            logger.info(f"\n{'='*60}")
            logger.info(f"å°è¯•: {url_info['name']}")
            logger.info(f"{'='*60}")
            
            success, response = self.make_request(url_info)
            
            if success:
                # æå–æ•°æ®
                data = self.extract_data_from_response(response)
                
                if data:
                    logger.info(f"  âœ… ä» {url_info['name']} æˆåŠŸæå–åˆ° {len(data)} æ¡æ•°æ®")
                    self.data_found = True
                    self.extracted_data = data
                    
                    # ä¿å­˜æ•°æ®
                    if self.save_to_csv(data):
                        return True
                    else:
                        logger.warning(f"  âš ï¸  æ•°æ®æå–æˆåŠŸä½†ä¿å­˜å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªURL")
                else:
                    logger.warning(f"  âš ï¸  è¯·æ±‚æˆåŠŸä½†æœªæå–åˆ°æ•°æ®ï¼Œå°è¯•ä¸‹ä¸€ä¸ªURL")
            else:
                logger.error(f"  âŒ è¯·æ±‚å¤±è´¥: {response}")
        
        # æ€»ç»“
        if self.data_found:
            logger.info("\nğŸ‰ çˆ¬è™«æ‰§è¡ŒæˆåŠŸ!")
            return True
        else:
            logger.error("\nğŸ˜ æ‰€æœ‰URLå°è¯•éƒ½æœªè·å–åˆ°æ•°æ®")
            logger.info("ğŸ’¡ å»ºè®®:")
            logger.info("  1. æ£€æŸ¥Cookieæ˜¯å¦æœ‰æ•ˆ")
            logger.info("  2. å¢åŠ è¯·æ±‚å»¶è¿Ÿ")
            logger.info("  3. ä½¿ç”¨ä»£ç†IP")
            logger.info("  4. æ›´æ¢User-Agent")
            return False

def main():
    """ä¸»å‡½æ•°"""
    crawler = AdvancedFOFACrawler("config.json")
    
    try:
        success = crawler.run()
        
        if success:
            logger.info("\nâœ… ç¨‹åºæ‰§è¡ŒæˆåŠŸ")
            sys.exit(0)
        else:
            logger.error("\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥")
            sys.exit(1)
        
    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ ç¨‹åºæ‰§è¡Œå¼‚å¸¸: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()