#!/usr/bin/env python3
"""
FOFAçˆ¬è™« - GitHub Actionsä¼˜åŒ–ç‰ˆ
ä¸“ä¸ºCIç¯å¢ƒä¼˜åŒ–ï¼Œå»é™¤äº¤äº’å¼è¾“å…¥
"""

import requests
import json
import os
import sys
import csv
import re
import base64
import time
from datetime import datetime
from urllib.parse import quote

class FOFACrawler:
    def __init__(self, config_file="config.json"):
        """åˆå§‹åŒ–çˆ¬è™«"""
        self.config_file = config_file
        self.config = self.load_config()
        self.data_found = False
        self.extracted_data = []
        
        # è®¾ç½®å®Œæ•´è¯·æ±‚å¤´
        self.full_headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'Cache-Control': 'max-age=0',
            'Connection': 'keep-alive',
            'DNT': '1',
            'Referer': 'https://en.fofa.info/',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36 Edg/144.0.0.0',
            'sec-ch-ua': '"Not(A:Brand";v="8", "Chromium";v="144", "Microsoft Edge";v="144"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'Cookie': self.config.get('cookies', '') if 'cookies' in self.config else ''
        }
        
        # è®¾ç½®ç®€å•è¯·æ±‚å¤´
        self.simple_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/144.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        }
        
        # å¸¸è§çš„æ— æ•ˆIPåˆ—è¡¨
        self.invalid_ips = [
            '1.1.1.1', '8.8.8.8', '8.8.4.4', '127.0.0.1', '0.0.0.0', 
            '255.255.255.255', '192.168.0.1', '192.168.1.1', '10.0.0.1',
            '172.16.0.1', '100.64.0.1', '169.254.0.1'
        ]
    
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not os.path.exists(self.config_file):
            print(f"âŒ é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨")
            return {}
        
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
            return config
        except Exception as e:
            print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return {}
    
    def encode_query(self, query_string):
        """å°†æŸ¥è¯¢å­—ç¬¦ä¸²ç¼–ç ä¸ºbase64æ ¼å¼"""
        try:
            query_bytes = query_string.encode('utf-8')
            base64_bytes = base64.b64encode(query_bytes)
            base64_string = base64_bytes.decode('utf-8')
            return base64_string
        except Exception as e:
            print(f"âŒ Base64ç¼–ç å¤±è´¥: {e}")
            return None
    
    def build_urls(self):
        """æ„å»ºURLåˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰"""
        urls = []
        
        # è·å–æŸ¥è¯¢å­—ç¬¦ä¸²å¹¶ç¼–ç 
        query_string = self.config.get('query_string', '')
        if not query_string:
            print("âŒ é…ç½®æ–‡ä»¶ä¸­æ²¡æœ‰æŸ¥è¯¢è¯­å¥")
            return urls
        
        base64_query = self.encode_query(query_string)
        if not base64_query:
            return urls
        
        # URLç¼–ç base64å­—ç¬¦ä¸²
        encoded_base64 = quote(base64_query, safe='')
        
        # æ„å»ºURLåˆ—è¡¨
        urls.append({
            'name': 'å¸¦å‚æ•°å’ŒCookieçš„è‹±æ–‡ç«™',
            'url': f"https://en.fofa.info/result?qbase64={encoded_base64}",
            'headers': self.full_headers,
            'has_cookie': True
        })
        
        urls.append({
            'name': 'å¸¦å‚æ•°ä¸å¸¦Cookieçš„è‹±æ–‡ç«™',
            'url': f"https://en.fofa.info/result?qbase64={encoded_base64}",
            'headers': self.simple_headers,
            'has_cookie': False
        })
        
        urls.append({
            'name': 'å¸¦å‚æ•°ä¸å¸¦Cookieçš„ä¸­æ–‡ç«™',
            'url': f"https://fofa.info/result?qbase64={encoded_base64}",
            'headers': self.simple_headers,
            'has_cookie': False
        })
        
        print(f"âœ… æ„å»ºäº† {len(urls)} ä¸ªURL")
        return urls
    
    def make_request(self, url_info):
        """å‘é€HTTPè¯·æ±‚"""
        print(f"\nğŸ“¡ å‘é€è¯·æ±‚åˆ°: {url_info['name']}")
        
        try:
            response = requests.get(
                url_info['url'], 
                headers=url_info['headers'], 
                timeout=self.config.get('settings', {}).get('timeout', 30),
                allow_redirects=True
            )
            
            print(f"  âœ… è¯·æ±‚å®Œæˆ!")
            print(f"    çŠ¶æ€ç : {response.status_code}")
            
            if response.status_code == 200:
                return True, response
            elif response.status_code == 403:
                return False, "è®¿é—®è¢«æ‹’ç» (403)"
            elif response.status_code == 401:
                return False, "éœ€è¦è®¤è¯ (401)"
            else:
                return False, f"HTTPé”™è¯¯: {response.status_code}"
                
        except requests.exceptions.Timeout:
            return False, "è¯·æ±‚è¶…æ—¶"
        except requests.exceptions.ConnectionError:
            return False, "è¿æ¥é”™è¯¯"
        except Exception as e:
            return False, f"è¯·æ±‚å¼‚å¸¸: {type(e).__name__}: {str(e)}"
    
    def extract_data_from_new_structure(self, html_content):
        """ä»æ–°çš„HTMLç»“æ„ä¸­æå–æ•°æ®"""
        print("    ä½¿ç”¨æ–°ç»“æ„è§£æ...")
        
        ip_port_pairs = []
        
        # æ–¹æ³•1: ç›´æ¥æŸ¥æ‰¾æ‰€æœ‰ hsxa-host ä¸­çš„é“¾æ¥
        host_pattern = r'<span class="hsxa-host"[^>]*>\s*<a[^>]*href="[^"]*"[^>]*>([^<]+)</a>'
        host_matches = re.findall(host_pattern, html_content)
        
        print(f"      ä»hsxa-hostæ‰¾åˆ° {len(host_matches)} ä¸ªåŒ¹é…")
        
        for host_text in host_matches:
            host_text = host_text.strip()
            if ':' in host_text:
                ip, port = host_text.split(':', 1)
                if self.is_valid_ip(ip):
                    ip_port_pairs.append([ip, port])
        
        # æ–¹æ³•2: ä»clipboardæ•°æ®æå–
        if not ip_port_pairs:
            print("      ä»clipboardæ•°æ®æå–...")
            copy_pattern = r'data-clipboard-text="([^"]+:\d+)"'
            copy_matches = re.findall(copy_pattern, html_content)
            
            for copy_text in copy_matches:
                if ':' in copy_text:
                    ip, port = copy_text.split(':', 1)
                    if self.is_valid_ip(ip):
                        ip_port_pairs.append([ip, port])
        
        # æ–¹æ³•3: åˆ†åˆ«æå–IPå’Œç«¯å£
        if not ip_port_pairs:
            print("      åˆ†åˆ«æå–IPå’Œç«¯å£...")
            
            # æå–IP
            ip_pattern = r'<a[^>]*class="hsxa-jump-a"[^>]*href="[^"]*qbase64=aXA=[^"]*"[^>]*>([^<]+)</a>'
            ip_matches = re.findall(ip_pattern, html_content)
            
            # æå–ç«¯å£
            port_pattern = r'<a[^>]*class="hsxa-port"[^>]*href="[^"]*qbase64=cG9ydD=[^"]*"[^>]*>([^<]+)</a>'
            port_matches = re.findall(port_pattern, html_content)
            
            # å‡è®¾IPå’Œç«¯å£æ˜¯æŒ‰é¡ºåºå¯¹åº”çš„
            min_count = min(len(ip_matches), len(port_matches))
            for i in range(min_count):
                ip = ip_matches[i].strip()
                port = port_matches[i].strip()
                
                if self.is_valid_ip(ip):
                    # ç¡®ä¿ç«¯å£æ˜¯æœ‰æ•ˆçš„æ•°å­—
                    if not port.isdigit():
                        port_match = re.search(r'(\d{1,5})', port)
                        port = port_match.group(1) if port_match else "443"
                    
                    ip_port_pairs.append([ip, port])
        
        return ip_port_pairs
    
    def extract_data_from_old_structure(self, html_content):
        """ä»æ—§çš„HTMLç»“æ„ä¸­æå–æ•°æ®"""
        print("    ä½¿ç”¨æ—§ç»“æ„è§£æ...")
        
        ip_port_pairs = []
        
        # æŸ¥æ‰¾æ‰€æœ‰çš„æ•°æ®æ¡ç›®å®¹å™¨
        item_pattern = r'<div class="hsxa-meta-data-item">(.*?)</div>\s*</div>\s*</div>\s*</div>'
        items = re.findall(item_pattern, html_content, re.DOTALL)
        
        print(f"      æ‰¾åˆ° {len(items)} ä¸ªæ•°æ®æ¡ç›®")
        
        for item_index, item_html in enumerate(items):
            # æå–IPåœ°å€
            ip_pattern = r'<a[^>]*href="[^"]*qbase64=aXA=[^"]*"[^>]*class="hsxa-jump-a"[^>]*>([^<]+)</a>'
            ip_matches = re.findall(ip_pattern, item_html, re.DOTALL)
            
            if ip_matches:
                ip = ip_matches[0].strip()
            else:
                # å¦‚æœæ‰¾ä¸åˆ°hsxa-jump-aï¼Œå°è¯•å…¶ä»–æ¨¡å¼
                ip_pattern2 = r'>\s*(\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})\s*<'
                ip_matches2 = re.findall(ip_pattern2, item_html)
                ip = ip_matches2[0].strip() if ip_matches2 else None
            
            # æå–ç«¯å£
            port_pattern = r'<a[^>]*href="[^"]*qbase64=cG9ydD=[^"]*"[^>]*class="hsxa-port"[^>]*>([^<]+)</a>'
            port_matches = re.findall(port_pattern, item_html, re.DOTALL)
            
            if port_matches:
                port = port_matches[0].strip()
            else:
                # å¦‚æœæ‰¾ä¸åˆ°hsxa-portï¼Œå°è¯•å…¶ä»–æ¨¡å¼
                port_pattern2 = r'port[^0-9]*(\d{1,5})'
                port_matches2 = re.search(port_pattern2, item_html, re.IGNORECASE)
                port = port_matches2.group(1) if port_matches2 else "443"
            
            # éªŒè¯IPå¹¶æ·»åŠ åˆ°åˆ—è¡¨
            if ip and self.is_valid_ip(ip):
                # ç¡®ä¿ç«¯å£æ˜¯æœ‰æ•ˆçš„æ•°å­—
                if not port.isdigit():
                    port_match = re.search(r'(\d{1,5})', port)
                    port = port_match.group(1) if port_match else "443"
                
                ip_port_pairs.append([ip, port])
        
        return ip_port_pairs
    
    def extract_table_data(self, html_content):
        """ä»è¡¨æ ¼ä¸­æå–IPå’Œç«¯å£æ•°æ®"""
        print("  æ­£åœ¨è§£æè¡¨æ ¼æ•°æ®...")
        
        # é¦–å…ˆå°è¯•æ–°ç»“æ„
        ip_port_pairs = self.extract_data_from_new_structure(html_content)
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•æ—§çš„è§£ææ–¹æ³•
        if not ip_port_pairs:
            ip_port_pairs = self.extract_data_from_old_structure(html_content)
        
        return ip_port_pairs
    
    def is_valid_ip(self, ip_str):
        """éªŒè¯IPåœ°å€æ˜¯å¦æœ‰æ•ˆä¸”ä¸æ˜¯å¸¸è§æ— æ•ˆIP"""
        # åŸºæœ¬IPæ ¼å¼éªŒè¯
        ip_pattern = r'^\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b$'
        if not re.match(ip_pattern, ip_str):
            return False
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ— æ•ˆIPåˆ—è¡¨ä¸­
        if self.config.get('settings', {}).get('filter_common_ips', True):
            if ip_str in self.invalid_ips:
                return False
        
        # æ£€æŸ¥æ¯ä¸ªéƒ¨åˆ†æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
        parts = ip_str.split('.')
        if len(parts) != 4:
            return False
        
        for part in parts:
            if not part.isdigit():
                return False
            num = int(part)
            if num < 0 or num > 255:
                return False
        
        # æ’é™¤ä¸€äº›ç‰¹æ®ŠIPæ®µ
        first_octet = int(parts[0])
        if first_octet == 0:  # 0.x.x.x
            return False
        if first_octet == 10:  # 10.x.x.x (å†…ç½‘)
            return False
        if first_octet == 100 and 64 <= int(parts[1]) <= 127:  # 100.64.x.x-100.127.x.x (è¿è¥å•†NAT)
            return False
        if first_octet == 127:  # 127.x.x.x (ç¯å›)
            return False
        if first_octet == 169 and int(parts[1]) == 254:  # 169.254.x.x (é“¾è·¯æœ¬åœ°)
            return False
        if first_octet == 172 and 16 <= int(parts[1]) <= 31:  # 172.16.x.x-172.31.x.x (å†…ç½‘)
            return False
        if first_octet == 192 and int(parts[1]) == 168:  # 192.168.x.x (å†…ç½‘)
            return False
        if first_octet == 198 and 18 <= int(parts[1]) <= 19:  # 198.18.x.x-198.19.x.x (æµ‹è¯•)
            return False
        
        return True
    
    def extract_data_from_response(self, response):
        """ä»å“åº”ä¸­æå–IPå’Œç«¯å£æ•°æ®"""
        print("\nğŸ” æ­£åœ¨æå–æ•°æ®...")
        
        html_content = response.text
        
        # å°è¯•è¡¨æ ¼è§£æ
        ip_port_pairs = self.extract_table_data(html_content)
        
        # å»é‡
        unique_pairs = []
        seen = set()
        
        for pair in ip_port_pairs:
            key = tuple(pair)
            if key not in seen:
                seen.add(key)
                unique_pairs.append(pair)
        
        print(f"  æ‰¾åˆ° {len(ip_port_pairs)} ä¸ªIPç«¯å£å¯¹ï¼Œå»é‡å {len(unique_pairs)} ä¸ª")
        
        # é™åˆ¶æœ€å¤§ç»“æœæ•°é‡
        max_results = self.config.get('settings', {}).get('max_results', 10)
        if len(unique_pairs) > max_results:
            unique_pairs = unique_pairs[:max_results]
            print(f"  é™åˆ¶ä¸ºå‰ {max_results} æ¡ç»“æœ")
        
        # æ˜¾ç¤ºç»“æœ
        if unique_pairs:
            print(f"\n  æ•°æ®é¢„è§ˆ:")
            for i, pair in enumerate(unique_pairs):
                print(f"    {i+1:2d}. IP: {pair[0]:15s} ç«¯å£: {pair[1]}")
        
        return unique_pairs
    
    def save_to_csv(self, data):
        """ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶"""
        if not data:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return False
        
        output_file = "results.csv"
        
        try:
            with open(output_file, 'w', newline='', encoding='utf-8-sig') as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(['IPåœ°å€', 'ç«¯å£'])
                writer.writerows(data)
            
            print(f"\nâœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            print(f"   å…±ä¿å­˜ {len(data)} æ¡è®°å½•")
            
            return True
        except Exception as e:
            print(f"\nâŒ ä¿å­˜CSVå¤±è´¥: {e}")
            return False
    
    def run(self):
        """è¿è¡Œçˆ¬è™«ä¸»é€»è¾‘"""
        print("=" * 60)
        print(f"FOFAçˆ¬è™« - GitHub Actionsç‰ˆ")
        print(f"è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
        
        # æ£€æŸ¥é…ç½®
        if not self.config:
            print("âŒ é…ç½®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
            return False
        
        # æ˜¾ç¤ºæŸ¥è¯¢è¯­å¥
        query_string = self.config.get('query_string', '')
        print(f"æŸ¥è¯¢è¯­å¥: {query_string}")
        
        # æ„å»ºURLåˆ—è¡¨
        urls = self.build_urls()
        if not urls:
            print("âŒ æ— æ³•æ„å»ºURLï¼Œè¯·æ£€æŸ¥æŸ¥è¯¢è¯­å¥")
            return False
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•URL
        for url_info in urls:
            print(f"\n{'='*60}")
            print(f"å°è¯•: {url_info['name']}")
            print(f"{'='*60}")
            
            success, response = self.make_request(url_info)
            
            if success:
                # æå–æ•°æ®
                data = self.extract_data_from_response(response)
                
                if data:
                    print(f"  âœ… ä» {url_info['name']} æˆåŠŸæå–åˆ° {len(data)} æ¡æ•°æ®")
                    self.data_found = True
                    self.extracted_data = data
                    
                    # ä¿å­˜æ•°æ®
                    if self.save_to_csv(data):
                        return True
                    else:
                        print(f"  âš ï¸  æ•°æ®æå–æˆåŠŸä½†ä¿å­˜å¤±è´¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ªURL")
                else:
                    print(f"  âš ï¸  è¯·æ±‚æˆåŠŸä½†æœªæå–åˆ°æ•°æ®ï¼Œå°è¯•ä¸‹ä¸€ä¸ªURL")
                    time.sleep(2)
            else:
                print(f"  âŒ è¯·æ±‚å¤±è´¥: {response}")
                time.sleep(2)
        
        # æ€»ç»“
        if self.data_found:
            print("\nğŸ‰ çˆ¬è™«æ‰§è¡ŒæˆåŠŸ!")
            return True
        else:
            print("\nğŸ˜ æ‰€æœ‰URLå°è¯•éƒ½æœªè·å–åˆ°æ•°æ®")
            return False

def main():
    """ä¸»å‡½æ•° - é’ˆå¯¹CIç¯å¢ƒä¼˜åŒ–"""
    crawler = FOFACrawler("config.json")
    
    try:
        success = crawler.run()
        
        if success:
            print("\nâœ… ç¨‹åºæ‰§è¡ŒæˆåŠŸ")
            sys.exit(0)  # æˆåŠŸé€€å‡ºç 
        else:
            print("\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥")
            sys.exit(1)  # å¤±è´¥é€€å‡ºç 
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¼‚å¸¸: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()